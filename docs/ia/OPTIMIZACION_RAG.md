# Optimizaci√≥n del Sistema RAG

## üìä An√°lisis de Rendimiento Actual

**Tiempo total por consulta: ~107 segundos**

### Desglose:
- ‚úÖ B√∫squeda vectorial: 2.09s (1.9%) - Excelente
- ‚ùå Generaci√≥n LLM: 105s (98.1%) - **CUELLO DE BOTELLA**

### Diagn√≥stico:
El modelo `qwen2.5:7b` (7.6B par√°metros, 4.7 GB) es demasiado lento para interacci√≥n en tiempo real.

---

## üöÄ Soluciones Recomendadas

### 1. **Cambiar a Modelo M√°s R√°pido** (Reducci√≥n estimada: 70-80%)

#### Opci√≥n A: Modelos peque√±os cuantizados
```bash
# Descargar modelo de 1.5B par√°metros (mucho m√°s r√°pido)
ollama pull qwen2.5:1.5b

# O usar versi√≥n 3B (balance calidad/velocidad)
ollama pull qwen2.5:3b
```

**Impacto esperado:**
- `qwen2.5:1.5b`: ~15-25s por consulta (70-75% m√°s r√°pido)
- `qwen2.5:3b`: ~35-50s por consulta (50-60% m√°s r√°pido)

#### Opci√≥n B: Modelos especializados en espa√±ol
```bash
# Modelo ligero optimizado para espa√±ol
ollama pull gemma2:2b
```

**Cambio en el c√≥digo:**
```python
# En pages/ai/chat_benchmarking.py, l√≠nea 22
rag = RAGChain(
    llm_model="qwen2.5:1.5b",  # Cambiar aqu√≠
    temperature=0.1,
    top_k=5
)
```

---

### 2. **Limitar Longitud de Respuestas** (Reducci√≥n: 20-30%)

Modificar el prompt para generar respuestas m√°s concisas.

**Cambio en el c√≥digo:**
```python
# En ai_modules/rag/rag_chain.py, l√≠nea 66
system_prompt = """Eres un asistente experto en descarbonizaci√≥n de la industria del cemento y concreto.

Usa el siguiente contexto para responder la pregunta de forma CONCISA (m√°ximo 3-4 oraciones).
Si no sabes la respuesta, di que no lo sabes.

Contexto:
{context}"""
```

**Impacto esperado:**
- Respuestas m√°s cortas = menos tokens generados = 20-30% m√°s r√°pido

---

### 3. **Reducir N√∫mero de Documentos Recuperados** (Reducci√≥n: 5-10%)

Actualmente recupera 5 documentos (`top_k=5`). Reducir a 3 acelera ligeramente.

**Cambio en el c√≥digo:**
```python
# En pages/ai/chat_benchmarking.py, l√≠nea 22
rag = RAGChain(
    llm_model="qwen2.5:7b",
    temperature=0.1,
    top_k=3  # Reducir de 5 a 3
)
```

**Impacto esperado:**
- Menos contexto = respuesta m√°s r√°pida
- Puede reducir precisi√≥n en preguntas complejas

---

### 4. **Usar Streaming de Respuestas** (Mejora percepci√≥n, no velocidad)

Mostrar la respuesta mientras se genera (como ChatGPT).

**Ventaja:**
- El usuario ve progreso inmediato
- Percepci√≥n de mayor rapidez
- No reduce tiempo total, pero mejora UX

**Implementaci√≥n:**
```python
# En pages/ai/chat_benchmarking.py
with st.chat_message("assistant"):
    message_placeholder = st.empty()
    full_response = ""

    # Streaming del LLM
    for chunk in rag.llm.stream(prompt_with_context):
        full_response += chunk
        message_placeholder.markdown(full_response + "‚ñå")

    message_placeholder.markdown(full_response)
```

---

### 5. **Cachear Respuestas Comunes** (Reducci√≥n: 100% en hits)

Guardar respuestas a preguntas frecuentes.

**Implementaci√≥n:**
```python
# Agregar cache en session_state
if "response_cache" not in st.session_state:
    st.session_state.response_cache = {}

# Antes de consultar RAG
cache_key = hash(prompt)
if cache_key in st.session_state.response_cache:
    result = st.session_state.response_cache[cache_key]
else:
    result = rag.query(prompt)
    st.session_state.response_cache[cache_key] = result
```

**Impacto:**
- Primera vez: mismo tiempo
- Consultas repetidas: instant√°neo

---

### 6. **Optimizar Configuraci√≥n de Ollama** (Reducci√≥n: 10-15%)

Ajustar par√°metros de inferencia:

```bash
# Aumentar hilos de CPU para Ollama
export OLLAMA_NUM_THREADS=8

# Usar GPU si est√° disponible (autom√°tico en Ollama)
# Verificar: ollama ps
```

**Configuraci√≥n en c√≥digo:**
```python
# En ai_modules/rag/rag_chain.py
self.llm = OllamaLLM(
    model=llm_model,
    base_url="http://localhost:11434",
    temperature=temperature,
    num_ctx=2048,  # Reducir contexto (default 4096)
    num_predict=256  # Limitar tokens generados
)
```

---

## üìù Recomendaci√≥n Final

### Implementaci√≥n por Fases:

**FASE 1 - Impacto Inmediato (70-80% mejora):**
1. Descargar `qwen2.5:1.5b`
2. Cambiar modelo en `chat_benchmarking.py`
3. Limitar longitud de respuestas en prompt

**Resultado esperado: 15-25 segundos por consulta**

**FASE 2 - Refinamiento (mejora adicional):**
1. Implementar streaming
2. Reducir `top_k` a 3
3. Agregar cach√© de respuestas

**Resultado esperado: 10-20 segundos + mejor UX**

**FASE 3 - Optimizaci√≥n Avanzada:**
1. Optimizar configuraci√≥n Ollama
2. Considerar usar GPU (CUDA/ROCm)
3. Implementar sistema de pre-carga de respuestas comunes

---

## üéØ Tabla Comparativa

| Configuraci√≥n | Tiempo/Consulta | Calidad | Implementaci√≥n |
|---------------|-----------------|---------|----------------|
| **Actual** (qwen2.5:7b) | ~107s | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ | ‚úÖ Actual |
| **qwen2.5:3b** + prompts cortos | ~35-40s | ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ | ‚ö° F√°cil |
| **qwen2.5:1.5b** + optimizaciones | ~15-20s | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | ‚ö° F√°cil |
| **1.5b** + streaming + cach√© | ~15s (percepci√≥n <5s) | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | üîß Media |

---

## üí° Pr√≥ximo Paso Sugerido

```bash
# 1. Descargar modelo m√°s r√°pido
ollama pull qwen2.5:3b

# 2. Modificar una l√≠nea de c√≥digo
# En pages/ai/chat_benchmarking.py, l√≠nea 22:
# llm_model="qwen2.5:3b"

# 3. Reiniciar Streamlit
# La app ya estar√° 60% m√°s r√°pida
```

**Nota:** Puedes probar diferentes modelos sin cambiar nada m√°s. Solo cambia el par√°metro `llm_model`.
